{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optical character recognition with an HHL support vector machine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The idea\n",
    "\n",
    "We want a full HHL-based support vector machine (SVM), which can identify certain characters. To keep this algorithm relatively easy to build, we will restrict ourselves to a SVM wich can decide whether a small binary (pure black and white) image displays the character '6' or '9'. At first, the image has to be proprocessed to extract a number of features. We will use the ratio between the number of black pixels between the left (upper) and right (lower) half of the image as our two features. With two training samples of '6' and '9' a SVM can be built which can classify any other written 6 or 9. We begin with the basic concept of the SVM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The (linear) support vector machine\n",
    "A support vecotr machine can classify input data into two groups. This training data is provided with the correspoding label of the group, hence the SVM is an instance of a supervised machine learning algorithm. The key idea is to find a hyperplane (for two feature dimensions, that is a line) that divides the feature space into two sections. \n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/b/b5/Svm_separating_hyperplanes_%28SVG%29.svg\" width=\"400px\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A bit of math\n",
    "Suppose you have training data set\n",
    "$$ (\\vec{x_1}, y_1), (\\vec{x_2}, y_2), \\dots, (\\vec{x_n}, y_n) $$ \n",
    "where $\\vec{x_i}$ is the vector in feature space and $y_i$ are either $1$ or $-1$ indicating the corresponding group.\n",
    "The goal is now to find a hyperplane, described with the normal-vector $\\vec{w}$ and an offset parameter $b$, that has the largest distance to any of the input points. This plane is therefore constructed by minimizing $|\\vec{w}|^2/2$ under the constraint $y_i (\\vec{x_i}\\cdot\\vec{w}+b) \\geq 1$.\n",
    "One can reformulate given optimalization problem with so called Kuhn-Tucker multipliers $\\vec{\\alpha}=(\\alpha_1, \\dots, \\alpha_n)^T$\n",
    "$$ L(\\vec{\\alpha}) = \\sum_j^n y_j\\alpha_j-\\frac{1}{2}\\sum_{j,k=1}^n\\alpha_jK_{jk}\\alpha_k$$\n",
    "with $K_{jk}=\\vec{x_j}\\cdot\\vec{x_k}$ and the constraints\n",
    "$$ \\sum_{j=1}^n\\alpha_j = 0, \\quad y_j\\alpha_j\\geq0. $$\n",
    "Therefore the parameters can be recovered with\n",
    "$$ \\vec{w} = \\sum_{j=1}^n\\alpha_j\\vec{x_j}, \\quad b = y_j-\\vec{w}\\cdot\\vec{x_j} \\,\\,\\,\\text{if}\\,\\, \\alpha_j \\neq 0$$\n",
    "only a small set of $\\alpha_j$s are non-zero and should give the same result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To classify a new input vector, just the $\\alpha_j$s are neccessary\n",
    "$$ y(\\vec{x}) = \\text{sign}\\left(\\sum_{j=1}^n  \\alpha_j \\vec{x_j}\\cdot\\vec{x}+b\\right). $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A least square approximation of the upper minimization problem leads to following linear equation\n",
    "$$ F \\begin{pmatrix}b\\\\\\vec{\\alpha}\\end{pmatrix} = \n",
    " \\begin{bmatrix} 0 & \\vec{1}^T \\\\\\vec{1} & K+\\gamma^{-1} \\mathbb{1}\\end{bmatrix} \\begin{pmatrix}b\\\\\\vec{\\alpha}\\end{pmatrix} = \\begin{pmatrix}0\\\\\\vec{y}\\end{pmatrix}, $$\n",
    " whose result will be the optimized parameters for classifying new inputs. $\\gamma$ is a user-specified parameter which determines the relative weight of the training error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The quantum support vector machine\n",
    "\n",
    "We have already seen that the HHL algorithm brings big advantage to problem of solving linear systems of equations. However, the output of the HHL is fully encoded in a quantum state, in our case\n",
    "$$ |b, \\vec{\\alpha}\\rangle = \\frac{1}{\\sqrt{C}}\\left(b|0\\rangle+\\sum_{j=1}^n\\alpha_j|j\\rangle\\right),$$\n",
    "but we cannot extract this information easily. However, it is possible to construct a query state for some data and calculate the classification on the quantum computer. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For that, we need a black box operation $U$ which is able to produce the state\n",
    "$$ \\frac{1}{\\sqrt{n}} \\sum_j^n |j\\rangle|0\\rangle \\overset{U}{\\rightarrow} \\frac{1}{\\sqrt{N}} \\sum_{j=1}^n \\left|\\vec{x_j}\\right|\\,|j\\rangle|\\vec{x_j}\\rangle. $$\n",
    "Applying $U$ to $|b, \\alpha\\rangle$ gives the state \n",
    "$$ |\\tilde{u}\\rangle = \\frac{1}{\\sqrt{N_\\tilde{u}}}\\left(b|0\\rangle|0\\rangle+\\sum_{j=1}^n \\alpha_j\\left|\\vec{x_j}\\right|\\,|j\\rangle|\\vec{x_j}\\rangle\\right). $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need another black box $V$ which prepares the query vector $\\vec{x}$\n",
    "$$ |\\tilde{x}\\rangle = \\frac{1}{\\sqrt{N_\\tilde{x}}}\\left(|0\\rangle|0\\rangle+\\sum_{j=1}^n \\alpha_j\\left|\\vec{x}\\right|\\,|j\\rangle|\\vec{x}\\rangle\\right). $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then we can calculate the dot product of $\\tilde{x}$ and $\\tilde{u}$\n",
    "$$ \\langle\\tilde{x}|\\tilde{u}\\rangle \\propto b + \\sum_{j=1}^n \\left|\\vec{x}\\right| \\left|\\vec{x_j}\\right| \\langle\\vec{x}|\\vec{x_j} \\rangle  = b + \\sum_{j=1}^n \\alpha_j\\vec{x}\\cdot\\vec{x_j}. $$ So the sign of this product gives the class of the query vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dot product can be calculated by a so-called swap test. Let $U$ and $V$ be controllable, then we can produce the state\n",
    "$$ \\frac{1}{\\sqrt{2}}(|0\\rangle|\\tilde{u}\\rangle + |1\\rangle|\\tilde{x}\\rangle). $$\n",
    "Applying the Hadamard gate on the first qubit and measuring it gives following probabilities for obtaining the state $|0\\rangle$\n",
    "$$ P_0 = \\frac{1}{2}(1+\\langle\\tilde{x}|\\tilde{u}\\rangle). $$\n",
    "So if more than half of the measurements result in $|0\\rangle$, The sign of $\\langle\\tilde{x}|\\tilde{u}\\rangle$ must be positive, therefore $\\vec{x}$ belongs to the group with $y_i = 1$.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Complete circuit looks like this"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
